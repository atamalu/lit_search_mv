---
title: "Literature Searches in R"
output: github_document
---

# Introduction

Note: Never replace an actual literature search with an automated method like this. Do not sacrifice quality for a small discount on time spent. This example is just to show how R can be used to assist in gathering some preliminary information.

# Gathering data

```{r, message=F, warning=F}
rm(list = ls())
library(rvest)
library(xml2)
library(rplos)

p.search <- searchplos(q = 'learned helplessness', limit = 10)
dois <- p.search$data$id
txts <- plos_fulltext(dois)
```

It is useful to save the data, so we will also do that

```{r}
saveRDS(txts, 'example.rds')
```

# Synthesizing data

When designing criteria for text mining the material you need, it helps a lot to have a background in the subject. In the case of a subject that's novel to you, it may actually be more helpful to do this <i>post</i> literature search.

### pt. 1: article title

Extracting the article title is relatively easy. It's the first entry labeled "article-title" and the alternative title is "alt-title". 

```{r}
txt <- read_xml(txts[[1]]) # read the xml object for first article

titl <- txt %>%
  html_nodes('article-title') %>% 
  xml_text() %>%
  .[[1]] # select first item (the actual title)

alt.titl <- txt %>%
  html_nodes('alt-title') %>% 
  xml_text() %>%
  .[[1]]
```

Now we turn it into a function

```{r}
get_article_titles <- function(article.object){
  
  txt <- read_xml(article.object)

  titl <- txt %>%
    html_nodes('article-title') %>% 
    xml_text() %>%
    .[[1]] 
  alt.titl <- txt %>%
    html_nodes('alt-title') %>% 
    xml_text() %>%
    .[[1]] 
  
  titls <- list(titl, alt.titl)
  return(titls)
  
}
```

### pt. 2: article classification

Mice are only used in certain labs for learned helplessness; rats are the primary subject species. So here we extract counts of ["mice" and "mouse"] vs ["rat" and "rats"] vs ["human", "people", "participant", "participants", "student", "students", "volunteer", "volunteers"] to classify the article. It should be noted that not every country uses the "subject" vs "participant" distinction, so this may not always be completely reliable.

We first extract a single article for our example before the loop. We can do this essentially the same way as extracting the article's titles.

```{r}
article <- txts[[1]] %>%
  read_xml() %>%
  html_nodes('p') %>% # extract in paragraphs
  xml_text()
```

Then classify the article as a participant (human) or subject (mouse/rat)

```{r}
library(stringr)

subject_or_part <- function(article.text){
  
  # count each
  rat <- str_count(article.text, pattern = 'rat|rats')
  mouse <- str_count(article.text, pattern = 'mice|mouse')
  ppl <- str_count(article.text, pattern = 'human|people|participant|participants|student|students|volunteer|volunteers')
  
  # move into list and keep highest count
  ret <- list(rat = sum(rat), 
              mouse = sum(mouse), 
              people = sum(ppl))

  ret <- ret[which.max(ret)]
  return(ret)
  
}

article.type <- names(subject_or_part(article))
```

This code counts the number of occurrences for these words, based on the aforementioned criteria. We have determined from the results that the species the first article uses is `r print(article.type)`.

### pt. 3: methodology

One important part of designing an experiment for fear conditioning or learned helplessness is shock intensity. Too strong of a stressor may result in physical damage that could confound neural activity measures, and too weak of a stressor will result in conditioned fear; not learned helplessness.

For psychological paradigms involving shock, you see shock intensity measured in milliamperes (mA). We need to gather <i>the numbers around</i> the suffix "mA". This is where regex is useful.

```{r}
### Extract text
intensity.list <- str_extract_all(string = article, 
                pattern = '[0-9.]{1,5} mA|[0-9.]{1,5}mA')
```

We can break down the expression as such.
`[0-9.]` numbers 0-9 or .
`[0-9.]{1,5}` any numbers 0-9 or ., that take up 1-5 spaces
`[0-9.]{1,5} mA` any numbers 0-9 or., that take up 1-5 spaces, that come behind a space and "mA"

We also include the same expression after the `|` sign (signifies "or") without a space for articles that may contain the unit directly after the number (e.g. 1.6mA)

Now we remove the empty items from our list 

```{r}
intensity.list <- Filter(length, intensity.list)
print(intensity.list)
```

and turn our code into a function.

```{r}
find_mA <- function(article.text){
  intensity.list <- str_extract_all(string = article.text, 
                pattern = '[0-9.]{1,5} mA|[0-9.]{1,5}mA')
  intensity.list <- Filter(length, intensity.list)
  
  return(intensity.list)
}
```

As you can see, we get many values instead of a single one. There are a few potential experimental explanations for this (e.g. multiple experiments, testing ranges) and should be looked into via actual reading. However, we have our shock intensity values extracted for this study which is good enough for exploration purposes.

# Applying functions

Now we need to apply the functions we wrote to every article we've gathered and put it into one place.

```{r}
### Return all objects as full text objects
all.txts <- lapply(txts, function(x){ 
  x %>%
    read_xml() %>%
    html_nodes('p') %>% # extract in paragraphs
    xml_text()
})

### Apply functions in loop and add to list
big.list <- list()

for(i in 1:length(all.txts)){
  
  subject.type <- names(subject_or_part(all.txts[[i]]))
  mA.list <- find_mA(all.txts[[i]])
  titls <- get_article_titles(txts[[1]])
  
  big.list[[i]] <- list(
    title = titls[[1]],
    alt.title = titls[[2]],
    subject.type = subject.type,
    mA.list = mA.list
  )
}

print(big.list)
```

This code takes the `txts` object and applies the function to each list item. Then, it creates a list and adds the results of the functions we created to it on each iteration; the number of which is the length of the `all.txts` object.

# Next up

Part 2 shows you how to visualize the data!
